# Project Description
This research investigates bidirectional text generation using the XLNet model and proposes a elongated Question Answering Model called BiGenXL and
compares its efficiency with other prominent language models such as GPT,BERT. While current transformers like GPT models have demonstrated remarkable capabilities 
in text generation, they are inherently unidirectional, generating text only from left to right. XLNet,on the other hand, employs a generalized autoregressive pretraining 
method that considers all permutations of the input sequence factorization order, enabling bidirectional text generation.

# Architecture
![Model Architecture](model_architecture)
